{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Boosting is a powerful machine learning technique that helps improve the accuracy of predictive models. It’s an ensemble method that combines multiple weak learners—typically decision trees—to create a strong classifier."
      ],
      "metadata": {
        "id": "NfxzRgeKnU41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Feature\tBagging\tBoosting\n",
        "Focus\tReduces variance\tReduces bias\n",
        "Training\tParallel\tSequential\n",
        "Model Combination\tMajority voting/averaging\tWeighted combination\n",
        "Complexity\tLess complex\tMore complex\n",
        "Risk of Overfitting\tLower"
      ],
      "metadata": {
        "id": "ueh_86MjnclT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How It Works:\n",
        "\n",
        "Assign Equal Weights: Initially, all training samples are given equal importance.\n",
        "\n",
        "Train a Weak Model: A simple model (often a decision tree with just one split, called a \"stump\") is trained on the data.\n",
        "\n",
        "Identify Errors: Misclassified samples are given higher weights, so the next model focuses on learning those correctly.\n",
        "\n",
        "Repeat Process: This process continues with each new model correcting mistakes made by the previous ones.\n",
        "\n",
        "Final Strong Model: At the end, all weak models are combined, with stronger models having more influence."
      ],
      "metadata": {
        "id": "tLZgg_panij0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Example: Classifying Apples and Oranges\n",
        "\n",
        "Imagine we want to build a model that classifies apples and oranges using features like color and shape.\n",
        "\n",
        "Step 1: Initialize Equal Weights\n",
        "We start with a dataset of apples and oranges, giving each data point equal importance.\n",
        "\n",
        "Step 2: Train the First Weak Model\n",
        "We train a simple classifier—say a decision stump (a shallow decision tree). It might predict that “red objects are apples and orange objects are oranges.”\n",
        "\n",
        "It correctly classifies most of the fruits.\n",
        "\n",
        "But it misclassifies some oranges that are slightly red.\n",
        "\n",
        "Step 3: Increase Weight of Misclassified Samples\n",
        "AdaBoost increases the weight of the misclassified oranges.\n",
        "\n",
        "This forces the next model to focus more on these difficult cases.\n",
        "\n",
        "Step 4: Train the Next Model\n",
        "The second weak classifier now learns a new rule: “If shape is round, classify as apple; if oval, classify as orange.”\n",
        "\n",
        "This fixes some of the previous errors.\n",
        "\n",
        "Step 5: Repeat the Process\n",
        "The model continues training weak classifiers in sequence, correcting mistakes made by previous models.\n",
        "\n",
        "Each new model gets a stronger ability to classify the apples and oranges correctly.\n",
        "\n",
        "Step 6: Combine All Weak Models\n",
        "AdaBoost assigns weights to all classifiers based on their accuracy.\n",
        "\n",
        "The final prediction is made using a weighted vote from all weak classifiers.\n",
        "\n",
        "Now, we have a strong classifier that can accurately distinguish apples and oranges."
      ],
      "metadata": {
        "id": "gzp-SmB2nsGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Gradient Boosting is an advanced boosting technique that builds models sequentially, minimizing errors using gradient descent. Instead of adjusting weights like AdaBoost, Gradient Boosting focuses on correcting residual errors left by previous models.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uw_YxXd6n5QK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. In Gradient Boosting, the loss function is a key component that guides the optimization process. The loss function measures the difference between actual and predicted values, and Gradient Boosting minimizes this error using gradient descent.\n"
      ],
      "metadata": {
        "id": "WZF-j14zoC55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\n",
        "Speed & Performance\tSlower, inefficient for large datasets\tOptimized for speed with parallelization\n",
        "Regularization\tLacks built-in regularization\tUses L1 (Lasso) & L2 (Ridge) regularization to prevent overfitting\n",
        "Tree Pruning\tUses depth-based tree growth\tPrunes trees using max depth & minimum loss reduction, improving generalization\n",
        "Handling Missing Values\tMay require data preprocessing\tAutomatically learns how to handle missing values\n",
        "Parallel Processing\tSequential, slower training\tUses multi-threaded execution for faster model training\n",
        "Custom Loss Functions\tLimited flexibility\tAllows user-defined objective functions & evaluation metrics"
      ],
      "metadata": {
        "id": "DERdc76XoLpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.\n",
        "Best For\tNumeric and structured data\tCategorical data-heavy datasets\n",
        "Handling Categorical Features\tRequires encoding (e.g., one-hot encoding)\tUses built-in categorical feature handling\n",
        "Training Speed\tFast, parallel computation\tOptimized for categorical data, can be faster in some cases\n",
        "Overfitting Prevention\tL1/L2 regularization\tOrdered boosting avoids target leakage\n",
        "Hyperparameter Tuning\tMore complex and requires careful tuning\tMore automatic tuning, requires fewer manual adjustments\n",
        "Missing Value Handling\tUses default values and special treatment\tAutomatically finds patterns in missing data\n",
        "Interpretability\tWidely used, many visualizations available\tProvides built-in feature importance insights"
      ],
      "metadata": {
        "id": "g7X4sEoToXeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Boosting techniques, like AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost, are widely used across industries due to their ability to improve predictive accuracy. Here are some impactful real-world applications:\n",
        "\n",
        "  1. Fraud Detection\n",
        "✔ Financial institutions use Boosting models to detect fraudulent transactions. ✔ XGBoost is commonly used for analyzing spending patterns and flagging anomalies in credit card transactions and online payments.\n",
        "\n",
        "  2. Medical Diagnosis & Healthcare Predictions\n",
        "✔ Boosting helps predict diseases based on patient symptoms and medical history. ✔ CatBoost, optimized for categorical data, is effective in cancer detection and predicting disease progression.\n",
        "\n",
        "  3. Financial Markets & Risk Assessment\n",
        "✔ Banks use Boosting models to assess loan default risks. ✔ Hedge funds leverage Gradient Boosting for stock market prediction and trend analysis.\n",
        "\n",
        "  4. Natural Language Processing (NLP)\n",
        "✔ Boosting techniques improve sentiment analysis, spam detection, and text classification. ✔ CatBoost and XGBoost enhance recommendation systems for personalized content suggestions.\n",
        "\n",
        "  5. Image Recognition & Computer Vision\n",
        "✔ Boosting is used in facial recognition systems (like in security and surveillance). ✔ AdaBoost was historically used in Viola-Jones face detection algorithm.\n",
        "\n",
        "  6. Customer Analytics & Recommendation Systems\n",
        "✔ E-commerce platforms use Boosting for product recommendation systems. ✔ CatBoost is effective in predicting customer churn by analyzing buying behavior.\n",
        "\n",
        "  7. Autonomous Vehicles & Self-Driving Cars\n",
        "✔ Boosting helps improve decision-making models for object detection, lane detection, and collision prevention.\n",
        "\n",
        "  8. Cybersecurity & Threat Detection\n",
        "✔ Boosting models detect malware, intrusion attempts, and phishing attacks in cybersecurity systems."
      ],
      "metadata": {
        "id": "62sW9oSqohKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How Regularization Improves XGBoost\n",
        "✔ Reduces Overfitting – Controls complexity, ensuring robust predictions. ✔ Improves Generalization – Helps models perform well on unseen data. ✔ Enhances Feature Selection – L1 regularization selects only the most relevant features. ✔ Smooths Decision Boundaries – L2 regularization prevents erratic predictions."
      ],
      "metadata": {
        "id": "_prko2Byo1oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Tuning hyperparameters is essential for optimizing Gradient Boosting models. Here are some key hyperparameters to adjust for better performance:\n",
        "\n",
        "  1. Learning Rate (eta)\n",
        "✔ Controls the step size in updating predictions. ✔ Lower values (e.g., 0.01–0.1) improve generalization but require more trees. ✔ Higher values can lead to overfitting if not carefully tuned.\n",
        "\n",
        "  2. Number of Trees (n_estimators)\n",
        "✔ Determines how many boosting rounds occur. ✔ More trees improve accuracy but increase computation time. ✔ Optimal balance needed to prevent overfitting.\n",
        "\n",
        "  3. Maximum Depth (max_depth)\n",
        "✔ Controls tree complexity. ✔ Shallower trees (3–6 depth) prevent overfitting. ✔ Deeper trees capture more patterns but risk overfitting.\n",
        "\n",
        "  4. Minimum Child Weight (min_child_weight)\n",
        "✔ Defines the minimum sum of weights required for a node split. ✔ Higher values lead to more conservative models, preventing overfitting. ✔ Lower values allow trees to grow deeper.\n",
        "\n",
        "  5. Subsampling (subsample)\n",
        "✔ Controls the percentage of training samples used per tree. ✔ Reduces variance by adding randomness. ✔ Typical range: 0.5–1.0.\n",
        "\n",
        "  6. Column Sampling (colsample_bytree, colsample_bylevel)\n",
        "✔ Selects a fraction of features at each level/tree. ✔ Helps improve generalization, especially in high-dimensional datasets.\n",
        "\n",
        "  7. Regularization (L1 & L2)\n",
        "✔ Prevents overfitting by penalizing large weights. ✔ L1 (Lasso) helps feature selection, while L2 (Ridge) smooths predictions.\n",
        "\n",
        "  8. Loss Function Selection\n",
        "✔ Different tasks require different loss functions:\n",
        "\n",
        "Regression → MSE, MAE\n",
        "\n",
        "Classification → Log Loss, Cross-Entropy\n",
        "\n",
        "  9. Gamma (min_split_loss)"
      ],
      "metadata": {
        "id": "WRwpX5btpAHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Feature Importance in Boosting refers to the relative contribution of each input feature in making predictions. It helps identify which features significantly impact the model's decision and which ones can be ignored or removed.\n",
        "\n",
        "How Feature Importance Works in Boosting\n",
        "Boosting algorithms like XGBoost, LightGBM, and CatBoost calculate feature importance using different methods:\n",
        "\n",
        "Split-Based Importance (Gain)\n",
        "\n",
        "Measures how much a feature improves the purity of splits in a tree.\n",
        "\n",
        "The higher the gain, the more impactful the feature.\n",
        "\n",
        "Used in XGBoost & LightGBM.\n",
        "\n",
        "Frequency-Based Importance\n",
        "\n",
        "Counts how often a feature is used for splitting across all trees.\n",
        "\n",
        "Features used more frequently are considered more important.\n",
        "\n",
        "Permutation Importance\n",
        "\n",
        "Randomly shuffles feature values and observes the drop in model performance.\n",
        "\n",
        "A large drop indicates high importance.\n",
        "\n",
        "SHAP Values (Shapley Additive Explanations)\n",
        "\n",
        "Provides an advanced method to measure a feature’s contribution for individual predictions.\n",
        "\n",
        "Used in explainable AI applications."
      ],
      "metadata": {
        "id": "ZNlCdAAHpSPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Key Reasons Why CatBoost Excels with Categorical Data\n",
        "Automatic Encoding of Categorical Features\n",
        "\n",
        "Traditional models require manual encoding (one-hot, label encoding), which can be inefficient.\n",
        "\n",
        "CatBoost natively transforms categorical variables, improving speed and accuracy.\n",
        "\n",
        "Ordered Boosting to Prevent Target Leakage\n",
        "\n",
        "Ensures that when learning patterns, it doesn’t use future data to predict past values.\n",
        "\n",
        "Helps avoid overfitting in datasets with many categorical features.\n",
        "\n",
        "Efficient GPU and CPU Implementation\n",
        "\n",
        "Optimized for both single-node and distributed environments.\n",
        "\n",
        "Works faster than XGBoost in many cases, especially with large categorical datasets.\n",
        "\n",
        "Handles Missing Values Automatically\n",
        "\n",
        "CatBoost doesn’t require manual imputation of missing values.\n",
        "\n",
        "Learns patterns even when some categorical fields have missing data.\n",
        "\n",
        "High Performance with Less Hyperparameter Tuning\n",
        "\n",
        "Compared to XGBoost and LightGBM, CatBoost requires less fine-tuning to achieve optimal results.\n",
        "\n",
        "Saves time when working on complex datasets."
      ],
      "metadata": {
        "id": "eRMCptXYpcHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "V1ZnaH_4pntm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "14. from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "cKPN2LiNppWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15. from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost Regressor\n",
        "model = AdaBoostRegressor(n_estimators=50, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")"
      ],
      "metadata": {
        "id": "up23XMGWp1G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16. from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "importance = model.feature_importances_\n",
        "\n",
        "# Print feature importance\n",
        "for name, imp in zip(feature_names, importance):\n",
        "    print(f\"{name}: {imp:.4f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(feature_names, importance)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Feature Importance in Gradient Boosting Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P5dgfokoqAS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate R-Squared Score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-Squared Score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "ya6M2WcyqIHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18. from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False)\n",
        "\n",
        "# Train models\n",
        "gb_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "gb_pred = gb_model.predict(X_test)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
        "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "ND7T0kA9qP3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=False)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "J6TRog2LqYvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "bpOZ3DOZqfmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost Classifier with a weak base estimator (Decision Tree)\n",
        "model = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "importance = model.feature_importances_\n",
        "\n",
        "# Print feature importance\n",
        "for name, imp in zip(feature_names, importance):\n",
        "    print(f\"{name}: {imp:.4f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "indices = np.argsort(importance)[::-1]\n",
        "plt.barh(np.array(feature_names)[indices], importance[indices])\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Feature Importance in AdaBoost Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wtI0Kp-Oq1dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "n_estimators = 200\n",
        "model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Arrays to store errors\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "# Train model iteratively and track errors\n",
        "for i in range(1, n_estimators + 1):\n",
        "    model.set_params(n_estimators=i)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Compute errors\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
        "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, n_estimators + 1), train_errors, label=\"Training Error\", color=\"blue\")\n",
        "plt.plot(range(1, n_estimators + 1), test_errors, label=\"Testing Error\", color=\"red\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Learning Curves for Gradient Boosting Regressor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TPVg3iaXq8F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "importance = model.feature_importances_\n",
        "\n",
        "# Print feature importance\n",
        "for name, imp in zip(feature_names, importance):\n",
        "    print(f\"{name}: {imp:.4f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "indices = np.argsort(importance)[::-1]\n",
        "plt.barh(np.array(feature_names)[indices], importance[indices])\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Feature Importance in XGBoost Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WgU6YLf8rEYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24. import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=False)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix for CatBoost Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kPhJsbjgrMze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different estimator values to test\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "\n",
        "# Store accuracy results\n",
        "accuracy_results = []\n",
        "\n",
        "# Train and evaluate models with different numbers of estimators\n",
        "for n in n_estimators_list:\n",
        "    model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                               n_estimators=n, learning_rate=1.0, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results.append(accuracy)\n",
        "    print(f\"n_estimators = {n}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs. number of estimators\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, accuracy_results, marker='o', linestyle='-', color='b', label='Accuracy')\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Impact of Number of Estimators on AdaBoost Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hi5FKqm7rV67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "26. import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC score\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC Curve (AUC = {auc_score:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Gradient Boosting Classifier\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fJDI90H5reN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "27. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize base XGBoost Regressor\n",
        "model = XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
        "\n",
        "# Perform GridSearchCV to find best learning rate\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best learning rate\n",
        "best_lr = grid_search.best_params_['learning_rate']\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "\n",
        "# Train final model with best learning rate\n",
        "final_model = XGBRegressor(n_estimators=100, learning_rate=best_lr, max_depth=3, random_state=42)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = final_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error with Best Learning Rate: {mse:.4f}\")\n",
        "\n",
        "# Plot learning rate vs. mean squared error\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(param_grid['learning_rate'], -grid_search.cv_results_['mean_test_score'], marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Learning Rate Optimization for XGBoost Regressor\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PL8rlrCSrnf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "28. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Generate an imbalanced classification dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier without class weights\n",
        "model_no_weights = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=False)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "f1_no_weights = f1_score(y_test, y_pred_no_weights)\n",
        "\n",
        "# Train CatBoost Classifier with class weights\n",
        "class_weights = {0: 1.0, 1: 5.0}  # Adjust weights based on class imbalance\n",
        "model_with_weights = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=False, class_weights=class_weights)\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "f1_with_weights = f1_score(y_test, y_pred_with_weights)\n",
        "\n",
        "# Print F1-Score comparison\n",
        "print(f\"F1-Score Without Class Weights: {f1_no_weights:.4f}\")\n",
        "print(f\"F1-Score With Class Weights: {f1_with_weights:.4f}\")\n",
        "\n",
        "# Bar plot comparison\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar([\"Without Weights\", \"With Weights\"], [f1_no_weights, f1_with_weights], color=[\"blue\", \"green\"])\n",
        "plt.xlabel(\"Model Type\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.title(\"Impact of Class Weighting in CatBoost\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sVQSWeabrvzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "29. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different learning rates to test\n",
        "learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "\n",
        "# Store accuracy results\n",
        "accuracy_results = []\n",
        "\n",
        "# Train and evaluate models with different learning rates\n",
        "for lr in learning_rates:\n",
        "    model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                               n_estimators=50, learning_rate=lr, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results.append(accuracy)\n",
        "    print(f\"Learning Rate = {lr}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs. learning rate\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(learning_rates, accuracy_results, marker='o', linestyle='-', color='b', label='Accuracy')\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Impact of Learning Rate on AdaBoost Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pSQNry--r5Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "30. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load Digits dataset (multi-class classification)\n",
        "data = load_digits()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier for multi-class classification\n",
        "model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, objective='multi:softprob')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for each class\n",
        "y_prob = model.predict_proba(X_test)\n",
        "\n",
        "# Compute Log-Loss\n",
        "logloss = log_loss(y_test, y_prob)\n",
        "print(f\"Log-Loss: {logloss:.4f}\")"
      ],
      "metadata": {
        "id": "pGWz--oZsB2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}